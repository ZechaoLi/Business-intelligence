
# 奇异值分解SVD的原理是怎样的，都有哪些应用场景
- 一般的方阵提取特征可以使用特征值分解，但当矩阵不是方阵的时候，就会需要用到奇异值分解，它能适用于任意的矩阵。

  SVD的基本公式为:  $A = U \Sigma V^{T}$

  其中$ A\in \mathbb{R}^{m+n} $, $ U\in \mathbb{R}^{m+m} $, $ \Sigma \in \mathbb{R}^{m+n} $且除了主对角线上的元素以外全为0，主对角线上的每个元素都称为奇异值，且已按大小拍好序，  $ V\in \mathbb{R}^{n+n} $。其中， $ U$ 的特征向量即是 $ AA^{T}$，一般我们将 $ U $ 中的每个特征向量叫做 $ A $ 的左奇异向量；$ V $ 的列向量即是 $ A^{T}A $ 的特征向量，一般我们将 $ V$ 中的每个特征向量叫做 $ A$ 的右奇异向量。

  SVD 可以用在推荐场景，把user和item的评分矩阵当成原始矩阵，对其做矩阵分解后，可以选定需要的特征值K来对矩阵降维，但是SVD分解需要矩阵是稠密的，所以需要对矩阵的缺失值做填充

  SVD还可以使用在图像领域，对普通图像做降为，仅仅需要少量的特征值和奇异矩阵就能还原出能够接受的图片，在存储上做优化

# funkSVD, BiasSVD，SVD++算法之间的区别是怎样的
- FunkSVD是在传统SVD面临计算效率问题时提出来的，既然将一个矩阵做SVD分解成3个矩阵很耗时，同时还面临稀疏的问题，那么我们能不能避开稀疏问题，同时只分解成两个矩阵呢？也就是说，现在期望我们的矩阵𝑀 这样进行分解：

$$
M_{m*n} = P^{T}_{m*k}Q_{k*n}
$$

​	  这里采用了线性回归的思想。我们的目标是让用户的评分和用矩阵乘积得到的评分残差尽可能的小，也就是说，可以用均方差作为损	  失函数，来寻找最终的𝑃 和 𝑄。

​	  BiasSVD是funkSVD的改进版，BiasSVD假设评分系统包括三部分的偏置因素：一些和用户物品无关的评分因素，用户有一些和物品	  无关的评分因素，称为用户偏置项。而物品也有一些和用户无关的评分因素，称为物品偏置项。这其实很好理解。比如一个垃圾山寨      	  货评分不可能高，自带这种烂属性的物品由于这个因素会直接导致用户评分低，与用户无关。

​	  SVD++算法在BiasSVD算法上进一步做了增强，它增加考虑用户的隐式反馈。

# 矩阵分解算法在推荐系统中有哪些应用场景，存在哪些不足
- SVD分解要求矩阵是稠密的，也就是说矩阵的所有位置不能有空白。有空白时我们的𝑀M是没法直接去SVD分解的。大家会说，如果这个矩阵是稠密的，那不就是说我们都已经找到所有用户物品的评分了嘛，那还要SVD干嘛! 的确，这是一个问题，传统SVD采用的方法是对评分矩阵中的缺失值进行简单的补全，比如用全局平均值或者用用户物品平均值补全，得到补全后的矩阵。接着可以用SVD分解并降维。



# 假设一个小说网站，有N部小说，每部小说都有摘要描述。如何针对该网站制定基于内容的推荐系统，即用户看了某部小说后，推荐其他相关的小说。原理和步骤是怎样的
- 基于内容的推荐系统不需要动态的用户行为，只要有内容就可以进行推荐。可以先对每个小说的摘要做分词，然后使用N-Gram，提取N个连续字的集合，作为特征，然后计算TF—IDF得到TFIDF矩阵，最后通过余弦相似度计算小说之间的相似度矩阵，对于指定的某部小说，推荐相似度最大的Top-K个小说


# Word2Vec的应用场景有哪些

- 计算商品的相似度、通过word2vec做embedding转换后的向量作为一个模型的输入、在社交网络中的推荐、向量快速检索
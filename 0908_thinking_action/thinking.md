
# ALS都有哪些应用场景
- ALS是交替最小二乘法，它的本质是一种数学优化技术，通过最小化误差和寻求数据的最佳函数匹配。
最主要的应用是用于推荐算法中的协同过滤算法，当使用user-item协同过滤时，就把用户对物品的评分矩阵做矩阵分解，分解为用户的因子矩阵和物品的因子矩阵，ALS则是求解矩阵分解问题的一种最优化方法，在每次迭代时，固定用户因子矩阵或者是物品因子矩阵中的一个，然后用固定的这个矩阵以及评级数据来更新另一个矩阵。之后，被更新的矩阵被固定住，再更新另外一个矩阵。如此迭代，直到模型收敛（或者是迭代了预设好的次数）。



# ALS进行矩阵分解的时候，为什么可以并行化处理
- 使用交替最小二乘法来求解。步骤是先固定一个X矩阵，然后求解另一个矩阵Y。然后再固定一个矩阵Y，求解另一个矩阵X。这就是交替二乘法的步骤。
  在矩阵求解的过程中，比如固定Y，求解X的话，目标评分矩阵A。X的每一行可以独立求解，X的第i行和Y的计算得到A的第i行。
  这样的话，对于每一步来说，X或者Y的行或者列都是可以独立并行求解的。这样ALS就可以进行并行化计算了。



# 梯度下降法中的批量梯度下降（BGD），随机梯度下降（SGD），和小批量梯度下降有什么区别（MBGD）
- 它们都是梯度下降的方法，最主要区别是批量梯度下降是对所有样本进行计算处理，其优点是全部数据确定的方向能更好的代表样本总体，从而更准确的找到收敛的方向，如果目标函数为凸函数是，一定能找到全局最优解，其缺点是当样本数量很大是，训练过程比较缓慢。随机梯度下降是指每次迭代只随机使用一个样本对参数进行更新，优点是参数更新速度快，缺点是准确度会有所下降，可能无法收敛且可能收敛与局部最优解，不利于并行实现。小批量梯度下降是批量梯度下降和随机梯度下降的一个折中的方法，每次使用batch_size个样本来进行参数更新。优点是速度并不会比SGD慢太多，同时减少收敛所需的迭代次数，效果能接近批量梯度下降的效果，且可实现并行。



# 你阅读过和推荐系统/计算广告/预测相关的论文么？有哪些论文是你比较推荐的，可以分享到微信群中
- 基于神经网络的点击率预测（deepCTR）论文比较推荐，例如wide & deep，deep & cross， deepFM，xDeepFM, FiBiNET等



